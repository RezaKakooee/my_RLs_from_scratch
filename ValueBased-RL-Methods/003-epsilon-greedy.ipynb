{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59afbbda-921a-42cb-8351-f6598a2bc20d",
   "metadata": {},
   "source": [
    "# $\\epsilon-greedy$ Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eea263-4d3e-491a-9b3a-d38cd4b56094",
   "metadata": {},
   "source": [
    "$\\epsilon-greedy$ is a popular exploration approach in which the agent for most of the time (with the probability of $1-\\epsilon$) exploit the current greedy action, and sometimes randomly explore an action among all available actions with the probability of $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fcb1c8-3e01-435f-9725-6ab3f8b85eae",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ecf385f-6bbb-49dd-b426-d1b14559f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac7f81-678d-4aa9-9374-3d97610a2f5a",
   "metadata": {},
   "source": [
    "## 1. Defining a Marketing Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f6d7a-615e-4b26-994d-e4dd8a1b3ff9",
   "metadata": {},
   "source": [
    "In this section, we want to define a hypothetical marketing scenario in which we have 5 ads each of which following a Bernoulli distribution. \n",
    "\n",
    "The goal is to find what is the best ad to show to the user.\n",
    "\n",
    "But before that, since each ad follows a Bernoulli distribution, let's first implement the Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711619c-db33-4bc4-8cda-6d7767ea0b6c",
   "metadata": {},
   "source": [
    "### 1.1. Defining Bernoulli Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "952b7044-bdc2-4562-9d40-1495f9a90359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bernoulli:\n",
    "    def __init__(self, p):\n",
    "        \"\"\"\n",
    "        Define a Bernoulli distribution\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p : a float number between 0 to 1\n",
    "            p represents the probability of choosing 1\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "        \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        Draw a single sample from the Bernoulli distribution\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward : binary: 0 or 1\n",
    "            A sample from the distribution\n",
    "        \"\"\"\n",
    "        reward = np.random.binomial(n=1, p=self.p)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12077be4-c8d4-4925-9bf1-9ee6eab8edd7",
   "metadata": {},
   "source": [
    "### 1.2. Defining $\\epsilon-greedy$ Algorithm Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1aa4c1a-c4e3-462f-89cd-c5d0d0dcaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    def __init__(self, options, epsilon):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        options : list\n",
    "            a list of distribution for each option. each option leads to a \n",
    "            probabilistic reward with unknown underlying distributions.\n",
    "        epsilon : float\n",
    "            exploration rate of the e-greedy algorithm\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.options = options\n",
    "        self.n_options = len(options)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self._reset()\n",
    "        self._build_model()\n",
    "        \n",
    "        \n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Define some variables to keep track of the reward and timestep.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "        self.avg_reward = 0\n",
    "        self.avg_rewards = []\n",
    "        self.time_step = 0\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build a tabular model to keep track of the action values and the\n",
    "        number of time each action has been selected.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.Q = np.zeros(self.n_options)\n",
    "        self.N = np.zeros(self.n_options)\n",
    "        \n",
    "        \n",
    "    def _get_action(self):\n",
    "        \"\"\"\n",
    "        Select an action according to e-greedy edxploration strategy\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            selected action to be executed in the env.\n",
    "\n",
    "        \"\"\"\n",
    "        if np.random.uniform() <= self.epsilon:\n",
    "            # explore non-greedy actions\n",
    "            action = np.random.randint(self.n_options)\n",
    "        else:\n",
    "            # exploit the greedy action\n",
    "            action = np.argmax(self.Q)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def _fit(self, action, rew):\n",
    "        \"\"\"\n",
    "        Based on the latest action and reward, we update the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            the selected action in the current timestep.\n",
    "        rew : float\n",
    "            the reward the env gives to the agent accordoing to it's selected action.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] += (1/self.N[action]) * (rew - self.Q[action])\n",
    "        \n",
    "        \n",
    "    def _update(self, rew):\n",
    "        \"\"\"\n",
    "        Updating the reward related variables and time_step in each timestep.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rew : float\n",
    "            the reward the env gives to the agent accordoing to it's selected action.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.rewards.append(rew)\n",
    "        self.total_reward += rew\n",
    "        self.avg_reward += (rew - self.avg_reward)/(self.time_step+1)\n",
    "        self.avg_rewards.append(self.avg_reward)\n",
    "        self.time_step += 1\n",
    "        \n",
    "        \n",
    "    def _step(self, action):\n",
    "        \"\"\"\n",
    "        Excuting the action the agent selected to see how rewarding it is.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "        the selected action of the current timestep\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        rew = self.options[action].draw()\n",
    "        self._fit(action, rew)\n",
    "        self._update(rew)\n",
    "    \n",
    "    \n",
    "    def play(self, n_iters):\n",
    "        \"\"\"\n",
    "        Running the algorithm for some iterations to see its performance\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_iters : int\n",
    "            number of iterations.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(n_iters):\n",
    "            action = self._get_action()\n",
    "            self._step(action)\n",
    "        self.best_action = np.argmax(self.Q)\n",
    "        \n",
    "            \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Printing the results.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        print(f\"----- e_greedy total_reward: {self.total_reward}\")\n",
    "        print(f\"----- e_greedy avg_reward: {self.avg_reward}\")\n",
    "        print(f\"----- e_greedy action_value: {self.Q}\")\n",
    "        print(f\"----- e_greedy n_visits_per_ad: {self.N}\")\n",
    "        print(f\"----- e_greedy best_action: {self.best_action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ad047-ffa3-43f8-9430-7c807e66bf90",
   "metadata": {},
   "source": [
    "## 2. Find the best add by $\\epsilon-greedy$ approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6203c0e6-d89d-46a3-b20a-be647d21e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define some ads following Bernoulli distribution/\n",
    "\n",
    "b_ad_A = Bernoulli(0.03)\n",
    "b_ad_B = Bernoulli(0.06)\n",
    "b_ad_C = Bernoulli(0.073)\n",
    "b_ad_D = Bernoulli(0.036)\n",
    "b_ad_E = Bernoulli(0.027)\n",
    "\n",
    "b_ads = [b_ad_A, b_ad_B, b_ad_C, b_ad_D, b_ad_E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a884fc79-5f4f-4bcc-82de-e0853cc11d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "464ab81a-96d0-4a89-b991-7cf5249bc93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- e_greedy total_reward: 6827\n",
      "----- e_greedy avg_reward: 0.06827000000000089\n",
      "----- e_greedy action_value: [0.03090172 0.06486165 0.07078318 0.03796204 0.02772277]\n",
      "----- e_greedy n_visits_per_ad: [ 1974.  3361. 90643.  2002.  2020.]\n",
      "----- e_greedy best_action: 2\n"
     ]
    }
   ],
   "source": [
    "e_greedy = EpsilonGreedy(b_ads, epsilon=0.1)\n",
    "e_greedy.play(n_iters=n_iters)\n",
    "e_greedy.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aad5f3-d66b-434e-80c6-b813d6b0ba15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
