{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c528636-da08-4177-9924-1543a2e346cd",
   "metadata": {},
   "source": [
    "# A/B/n Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe689b-690a-404e-bd56-2a80fb5f70aa",
   "metadata": {},
   "source": [
    "A/B/n testing is an exploration approach to find the best option among a set of alternatives each of which following an unknown distribution.\n",
    "\n",
    "The way A/B/n testing finds the best option is simply sample almost uniformly from all available alternatives. \n",
    "\n",
    "Then the best option is the one that led to the highest reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379f085-1546-414f-8f4c-94c7c86e3306",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51409d12-9236-4c1c-97c8-71f62cf0f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20265b0-1e1e-4d31-b98d-3683786a4d96",
   "metadata": {},
   "source": [
    "## 1. Defining a Marketing Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d46500-7bda-468d-9f3d-2ceecd35677b",
   "metadata": {},
   "source": [
    "In this section, we want to define a hypothetical marketing scenario in which we have 5 ads each of which following a Bernoulli distribution. \n",
    "\n",
    "The goal is to find what is the best ad to show to the user.\n",
    "\n",
    "But before that, since each ad follows a Bernoulli distribution, let's first implement the Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f9ae6-0729-4029-97e9-7370925f69b2",
   "metadata": {},
   "source": [
    "### 1.1. Defining Bernoulli Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291a87c7-0bd3-46f3-b82e-59e43be40a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bernoulli:\n",
    "    def __init__(self, p):\n",
    "        \"\"\"\n",
    "        Define a Bernoulli distribution\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p : a float number between 0 to 1\n",
    "            p represents the probability of choosing 1\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "        \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        Draw a single sample from the Bernoulli distribution\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward : binary: 0 or 1\n",
    "            A sample from the distribution\n",
    "        \"\"\"\n",
    "        reward = np.random.binomial(n=1, p=self.p)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c0c0f-9caa-4d63-bb0c-029a7bea9579",
   "metadata": {},
   "source": [
    "### 1.2. Defining A/B Testing Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af9a7e-1e28-4931-8e96-a72f51428332",
   "metadata": {},
   "source": [
    "Now, we can define the bandit game class which receives some distributions and allows the user to pull them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc225811-b99c-46ef-8835-592332b59d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABnTesting:\n",
    "    def __init__(self, ads):\n",
    "        self.ads = ads\n",
    "        self.n_ads = len(ads)\n",
    "        \n",
    "        self._reset()\n",
    "        self._build_model()\n",
    "        \n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Define some variables to keep track of the reward and timestep.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "        self.avg_reward = 0\n",
    "        self.avg_rewards = []\n",
    "        self.time_step = 0\n",
    "        \n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build a tabular model to keep track of the action values and the\n",
    "        number of time each action has been selected.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.Q = np.zeros(self.n_ads) # action value \n",
    "        self.N = np.zeros(self.n_ads) # action frequency\n",
    "        \n",
    "        \n",
    "    def _fit(self, action, rew):\n",
    "        \"\"\"\n",
    "        Based on the latest action and reward, we update the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            the selected action in the current timestep.\n",
    "        rew : float\n",
    "            the reward the env gives to the agent accordoing to it's selected action.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] += 1/self.N[action] * (rew - self.Q[action])\n",
    "        \n",
    "    \n",
    "    def _update(self, rew):\n",
    "        \"\"\"\n",
    "        Updating the reward related variables and time_step in each timestep.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rew : float\n",
    "            the reward the env gives to the agent accordoing to it's selected action.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.rewards.append(rew)\n",
    "        self.total_reward += rew\n",
    "        self.avg_reward += (rew - self.avg_reward)/(self.time_step+1)\n",
    "        self.avg_rewards.append(self.avg_reward)\n",
    "        self.best_action = np.argmax(self.Q)\n",
    "        self.time_step += 1\n",
    "        \n",
    "        \n",
    "    def _step(self, action):\n",
    "        \"\"\"\n",
    "        Excuting the action the agent selected to see how rewarding it is.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "        the selected action of the current timestep\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        rew = self.ads[action].draw()\n",
    "        self._fit(action, rew)\n",
    "        self._update(rew)\n",
    "            \n",
    "    \n",
    "    def train(self, n_iters=10e6):\n",
    "        \"\"\"\n",
    "        in the training phase of the A/Bn testing we simply loop over all \n",
    "        available actions. \n",
    "        But, in the test phase, we always select the best_action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_iters : int, optional\n",
    "            Number of training iterations. The default is 10e6.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.phase_name = 'train'\n",
    "        \n",
    "        for i in range(n_iters):\n",
    "            action = np.random.randint(self.n_ads)\n",
    "            self._step(action)\n",
    "        \n",
    "        \n",
    "    def test(self, best_action, n_iters=100):\n",
    "        \"\"\"\n",
    "        In the test phase, we always select the best action. So, we need to \n",
    "        know what the best action is.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        best_action : TYPE\n",
    "            the best action learned in the training phase.\n",
    "        n_iters : TYPE, optional\n",
    "            Number of test iterations. The default is 100.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.phase_name = 'test'\n",
    "        \n",
    "        self._reset()\n",
    "        for i in range(n_iters):\n",
    "            self._step(best_action)\n",
    "        \n",
    "        \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Printing the results.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        print(f\"\\n----- abn total_reward: {self.total_reward}\")\n",
    "        print(f\"----- abn avg_reward: {self.avg_reward}\")\n",
    "        print(f\"----- abn best_action: {self.best_action}\")\n",
    "        if self.phase_name == 'train':\n",
    "            print(f\"----- abn action_value: {self.Q}\")\n",
    "            print(f\"----- abn n_visits_per_ad: {self.N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f54341-c022-4dd5-a846-221994cdd688",
   "metadata": {},
   "source": [
    "## Train and Test the A/B Testing Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2be59b-5a97-48b8-9408-d705422ca43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define some ads following Bernoulli distribution/\n",
    "\n",
    "b_ad_A = Bernoulli(0.03)\n",
    "b_ad_B = Bernoulli(0.06)\n",
    "b_ad_C = Bernoulli(0.073)\n",
    "b_ad_D = Bernoulli(0.036)\n",
    "b_ad_E = Bernoulli(0.027)\n",
    "\n",
    "b_ads = [b_ad_A, b_ad_B, b_ad_C, b_ad_D, b_ad_E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99010e66-7b15-4756-821d-827c38a80999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- abn total_reward: 467\n",
      "----- abn avg_reward: 0.04670000000000006\n",
      "----- abn best_action: 2\n",
      "----- abn action_value: [0.03178739 0.0629648  0.07081749 0.04057524 0.02533532]\n",
      "----- abn n_visits_per_ad: [1919. 2017. 2104. 1947. 2013.]\n"
     ]
    }
   ],
   "source": [
    "## Instantiate and train the A/B testing model\n",
    "\n",
    "n_iters = 100_000\n",
    "\n",
    "abn_tester = ABnTesting(b_ads)\n",
    "\n",
    "abn_tester.train(n_iters=int(0.1*n_iters))\n",
    "\n",
    "abn_tester.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dd93cec-0002-42c3-a21b-ada83aca00d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- abn total_reward: 8\n",
      "----- abn avg_reward: 0.08888888888888888\n",
      "----- abn best_action: 2\n"
     ]
    }
   ],
   "source": [
    "## Test the trained A/B testing model\n",
    "\n",
    "n_iters = 100\n",
    "\n",
    "abn_tester.test(best_action=abn_tester.best_action, n_iters=int(0.9*n_iters))\n",
    "\n",
    "abn_tester.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51167c-4fd2-4593-aad1-f71f6ab73b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8d014-1f95-41ac-b6d5-1fa6f9f92588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
